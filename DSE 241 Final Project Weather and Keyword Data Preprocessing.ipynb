{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d167525",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/joythompson/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/joythompson/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "#import required functions\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import math\n",
    "import os\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import datetime\n",
    "from pandas import DataFrame\n",
    "import itertools\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "import torch\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import re\n",
    "import spacy\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "from bs4 import BeautifulSoup\n",
    "import mechanize\n",
    "import urllib.request\n",
    "import http.cookiejar as cookielib\n",
    "import requests\n",
    "from pprint import pprint\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea3abc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createfiledirectory(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print ('Error: Creating directory. ' +  directory)\n",
    "    return directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41eb27e1",
   "metadata": {},
   "source": [
    "# **Analyze Disneyland Reviews Raw Dataset Column Attributes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3bdc23ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Worksheet index 0 is invalid, 0 worksheets found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zt/sk2gv8351kd6jdnkzs4n4sy80000gn/T/ipykernel_11850/819574426.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtabready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreatefiledirectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojdir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'ProcessedTableauReady/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mreviews\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtabraw\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'KaggleDisneylandReviews_excel.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mnumofregion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Reviewer_Location'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         data = io.parse(\n\u001b[0m\u001b[1;32m    373\u001b[0m             \u001b[0msheet_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msheet_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, sheet_name, header, names, index_col, usecols, squeeze, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds)\u001b[0m\n\u001b[1;32m   1270\u001b[0m             \u001b[0mDataFrame\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mExcel\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m         \"\"\"\n\u001b[0;32m-> 1272\u001b[0;31m         return self._reader.parse(\n\u001b[0m\u001b[1;32m   1273\u001b[0m             \u001b[0msheet_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msheet_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m             \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, sheet_name, header, names, index_col, usecols, squeeze, dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds)\u001b[0m\n\u001b[1;32m    535\u001b[0m                 \u001b[0msheet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sheet_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masheetname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# assume an integer if not a string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m                 \u001b[0msheet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sheet_by_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masheetname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sheet_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msheet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/pandas/io/excel/_openpyxl.py\u001b[0m in \u001b[0;36mget_sheet_by_index\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_sheet_by_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_if_bad_sheet_by_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworksheets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mraise_if_bad_sheet_by_index\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0mn_sheets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msheet_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mn_sheets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    469\u001b[0m                 \u001b[0;34mf\"Worksheet index {index} is invalid, {n_sheets} worksheets found\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: Worksheet index 0 is invalid, 0 worksheets found"
     ]
    }
   ],
   "source": [
    "username='joythompson'\n",
    "projdir=\"/Users/\"+username+\"/Desktop/DSE241_FinalProject_JoyT/\"\n",
    "tabraw=createfiledirectory(projdir + 'PreprocessedData/')\n",
    "tabready=createfiledirectory(projdir + 'ProcessedTableauReady/')\n",
    "\n",
    "reviews=pd.read_excel(tabraw+'KaggleDisneylandReviews_excel.xlsx')\n",
    "                           \n",
    "numofregion=len(reviews.groupby(['Reviewer_Location']).count().reset_index())\n",
    "maxrevlen=0\n",
    "maxloclen=0\n",
    "minrevlen=500\n",
    "minloclen=5\n",
    "for ind in reviews.index:\n",
    "    if len(str(reviews.loc[ind, 'Review_Text']))>maxrevlen:\n",
    "        maxrevlen=len(str(reviews.loc[ind, 'Review_Text']))\n",
    "    elif len(str(reviews.loc[ind, 'Review_Text']))!=0 and len(str(reviews.loc[ind, 'Review_Text']))<minrevlen:\n",
    "        minrevlen=len(str(reviews.loc[ind, 'Review_Text']))\n",
    "    if len(str(reviews.loc[ind, 'Reviewer_Location']))>maxloclen:\n",
    "        maxloclen=len(str(reviews.loc[ind, 'Reviewer_Location']))\n",
    "    elif len(str(reviews.loc[ind, 'Reviewer_Location']))!=0 and len(str(reviews.loc[ind, 'Reviewer_Location']))<minloclen:\n",
    "        minloclen=len(str(reviews.loc[ind, 'Reviewer_Location']))\n",
    "print('# of Unique Countries: '+str(numofregion)+' Max Char Review Text: '+str(maxrevlen)+' Max Char Location: '+\n",
    "      str(maxloclen)+' Min Char Review Text: '+str(minrevlen)+' Min Char Location: '+str(minloclen))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ab970e",
   "metadata": {},
   "source": [
    "# **Extract Monthly Temperature & Humidity Historical Data from Timeanddate.com for All Branch Locations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de569ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getweatherdata(linkloc, branchdata, country):  \n",
    "    for year in range(2010, 2020):\n",
    "        for month in range(1, 13):\n",
    "            month=str(month)\n",
    "            year=str(year)\n",
    "            url=linkloc + month + '&year='+ year\n",
    "            response =urllib.request.Request(url)\n",
    "            result = urllib.request.urlopen(response)\n",
    "            resulttext = result.read()\n",
    "            soup = BeautifulSoup(resulttext)\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.extract()    # rip it out\n",
    "            text = soup.get_text()\n",
    "            lines = (line.strip() for line in text.splitlines())\n",
    "            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "            text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "            temptext=text.split('TemperatureHumidityPressureHigh')[1].split('Reported')[0]\n",
    "            temphigh=temptext.split('째')[0]\n",
    "            humidhigh=temptext.split(')')[1].split('%')[0]\n",
    "            templow=temptext.split('Low')[1].split('째')[0]\n",
    "            humidlow=temptext.split('Low')[1].split(')')[1].split('%')[0]\n",
    "            tempavg=temptext.split('Average')[1].split('째')[0]\n",
    "            humidavg=temptext.split('Average')[1].split('째F')[1].split('%')[0]\n",
    "            if int(year)>2010 or int(month)>9:\n",
    "                branchdata=branchdata.append({'location':country, 'month':month, 'year':year,'temp_high':temphigh, 'temp_low':templow, 'temp_average':tempavg,\n",
    "                                          'humid_high':humidhigh,  'humid_low':humidlow, 'humid_average':humidavg}, ignore_index=True)\n",
    "    return branchdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ab33099",
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.timeanddate.com/weather/france/paris/historic?month=5&year=2012'\n",
    "response =urllib.request.Request(url)\n",
    "result = urllib.request.urlopen(response)\n",
    "resulttext = result.read()\n",
    "soup = BeautifulSoup(resulttext)\n",
    "for script in soup([\"script\", \"style\"]):\n",
    "    script.extract()    # rip it out\n",
    "text = soup.get_text()\n",
    "lines = (line.strip() for line in text.splitlines())\n",
    "chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "text = '\\n'.join(chunk for chunk in chunks if chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed89dfbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    }
   ],
   "source": [
    "print(text.find('rain'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4cca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "branchdata=pd.DataFrame(columns=['location','month', 'year' 'temp_high', 'temp_low', 'temp_average', 'humid_high', 'humid_low', 'humid_average'])\n",
    "branchdata=getweatherdata('https://www.timeanddate.com/weather/france/paris/historic?month=', branchdata, 'Paris')\n",
    "branchdata=getweatherdata('https://www.timeanddate.com/weather/usa/anaheim/historic?month=', branchdata, 'Anaheim')\n",
    "branchdata=getweatherdata('https://www.timeanddate.com/weather/@1818609/historic?month=', branchdata, 'Hong Kong')\n",
    "branchdata.to_csv(tabraw+'BranchMonthlyWeatherData.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed8d5f6",
   "metadata": {},
   "source": [
    "# **Append Monthly Temperature & Humidity Data According to Branch & Visit Month to Reviews Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d745be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addtemphumid(reviews, branchdata, city):  \n",
    "    data=branchdata[branchdata['location']==city].set_index('year_mon')\n",
    "    reviews.loc[ind, 'location']=city\n",
    "    reviews.loc[ind, 'temp_average']=data.loc[yearmon, 'temp_average']\n",
    "    reviews.loc[ind, 'temp_low']=data.loc[yearmon, 'temp_low']\n",
    "    reviews.loc[ind, 'temp_high']=data.loc[yearmon, 'temp_high']\n",
    "    reviews.loc[ind, 'humid_average']=data.loc[yearmon, 'humid_average']\n",
    "    reviews.loc[ind, 'humid_low']=data.loc[yearmon, 'humid_low']\n",
    "    reviews.loc[ind, 'humid_high']=data.loc[yearmon, 'humid_high']\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f01a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "branchdata=pd.read_csv(tabraw+'BranchMonthlyWeatherData.csv')\n",
    "for ind in branchdata.index:\n",
    "    branchdata.loc[ind, 'year_mon']=str(branchdata.loc[ind,'year'])+ '-'+str(branchdata.loc[ind,'month'])\n",
    "droplist=[]\n",
    "for ind in reviews.index:\n",
    "    yearmon=reviews.loc[ind, 'Year_Month']\n",
    "    if str(yearmon) in ['2010-9', '2010-8','2010-7', '2010-6', '2010-5', 'missing','2010-4','2010-3','2010-2','2010-1']:\n",
    "        droplist.append(ind)\n",
    "    elif 'Paris' in reviews.loc[ind, 'Branch']:\n",
    "        reviews=addtemphumid(reviews, branchdata, 'Paris')\n",
    "    elif 'HongKong' in reviews.loc[ind, 'Branch']:\n",
    "        reviews=addtemphumid(reviews, branchdata, 'Hong Kong')\n",
    "    elif 'California' in reviews.loc[ind, 'Branch']:\n",
    "        reviews=addtemphumid(reviews, branchdata, 'Anaheim')\n",
    "review=reviews.drop(dropd\n",
    "review.to_csv(tabready+'DisneyReviewerAppendWeatherData.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ec9196",
   "metadata": {},
   "source": [
    "# **Create Dataset of Top Unique Keywords/Phrases from Reviewers Comments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22286816",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "wordlabelpd=pd.DataFrame(columns=['Review_ID', 'EntityOrderID', 'Text', 'CategoryLabel']);\n",
    "for ind in review.index:\n",
    "    doc = nlp(str(review.loc[ind,'Review_Text']))\n",
    "    entorder=0;\n",
    "    for ent in doc.ents:\n",
    "        entorder=entorder+1;\n",
    "        label=ent.label_;\n",
    "        wordlabelpd=wordlabelpd.append({'Review_ID':review.loc[ind, 'Review_ID'],\n",
    "                'EntityOrderID':entorder , 'Text': str(ent.text).lower(), 'CategoryLabel': label}, ignore_index=True);\n",
    "\n",
    "uniquepd=wordlabelpd.groupby(['Text']).count().reset_index().sort_values(by=['Review_ID'], ascending=False)\n",
    "uniquepd.to_csv(tabraw+'ExtractWordsPhrasesReviewComment.csv')\n",
    "wordlabelpd.to_csv(tabraw+'CreateUniqueWordListExtractedReviewComment.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9261f8",
   "metadata": {},
   "source": [
    "# **Create Dataset of All Cases a Top Unique Keyword/Phrases Exists in a Rewiewers Comments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060af057",
   "metadata": {},
   "outputs": [],
   "source": [
    "#File uniquepd.csv manually cleaned and limited to 100 with excel to create top 100 words.csv\n",
    "#Ideally string matching & data cleaning functions should be utilized to create the top word list\n",
    "top=pd.read_csv(tabraw+'top 100 words.csv').drop(columns=['Unnamed: 0'])\n",
    "review=pd.read_csv(tabraw+'DisneyReviewerAppendWeatherData.csv')\n",
    "topwordall=pd.DataFrame(columns=['Top Word','Work Top Ranking', '# of Reviews w/Word','Review_ID', 'Rating', 'Year_Month', 'Reviewer_Location', \n",
    "       'Branch', 'location', 'temp_average', 'temp_low', 'temp_high','humid_average', 'humid_low', 'humid_high'])\n",
    "\n",
    "for ind in top.index:\n",
    "    text=top.loc[ind, 'Text']\n",
    "    for i in range(1,6):\n",
    "        top.loc[ind, 'Rating '+ str(i)]=0\n",
    "    numofrate=0\n",
    "    sumrate=0\n",
    "    for ind2 in review.index:\n",
    "        revtext=review.loc[ind2, 'Review_Text']\n",
    "        rating=review.loc[ind2, 'Rating']\n",
    "        if text in revtext:\n",
    "            topwordall=topwordall.append({'Top Word':text, '# of Reviews w/Word':top.loc[ind, '# of Reviews w/Word'],\n",
    "                                          'Work Top Ranking':ind+1, 'Review_ID': review.loc[ind2, 'Review_ID'], \n",
    "                                          'Rating':rating, 'Year_Month':review.loc[ind2, 'Year_Month'], \n",
    "                                          'Reviewer_Location':review.loc[ind2, 'Reviewer_Location'], \n",
    "                                           'Branch':review.loc[ind2, 'Branch'], 'location':review.loc[ind2, 'location'],\n",
    "                                          'temp_average':review.loc[ind2, 'temp_average'],'temp_low':review.loc[ind2, 'temp_low'], \n",
    "                                          'temp_high': review.loc[ind2, 'temp_high'],'humid_average':review.loc[ind2, 'humid_average'],'humid_low':review.loc[ind2, 'humid_low'], \n",
    "                                          'humid_high': review.loc[ind2, 'humid_high']\n",
    "                                         }, ignore_index=True)\n",
    "            numofrate=numofrate+1\n",
    "            sumrate=sumrate+rating\n",
    "            top.loc[ind, 'Rating '+ str(int(rating))]=top.loc[ind, 'Rating '+ str(int(rating))]+1\n",
    "    top.loc[ind, 'Rating Average']=np.round(sumrate/numofrate, decimals=2)\n",
    "top.to_csv(tabready+'top_100_words_with_avg_rating.csv')\n",
    "topwordall.to_csv(tabready+'top_100_words_with_all_review_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
